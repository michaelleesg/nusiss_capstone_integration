\section{Agent A: Ingestion \& Normalization}
\label{sec:agentA}

\subsection{Objective}
Agent A continuously acquires, verifies, and normalizes cyber threat intelligence (CTI) from heterogeneous sources into a stable JSON schema for downstream processing (Agents B--D). It is designed for repeatability, rate-limited politeness, de-duplication, and traceable provenance.

\subsection{Sources \& Feed Adapters}
We support the following feed types:
\begin{itemize}
  \item \textbf{Structured APIs}: CVE JSON (official CVE Services) \cite{cve_reference}, EPSS API \cite{epss_reference}, CISA KEV catalog \cite{cisa_kev}.
  \item \textbf{Semi-structured}: Vendor advisories (RSS/Atom/JSON), CERT bulletins, blog posts.
  \item \textbf{Unstructured}: Security research articles, incident write-ups; optionally converted from HTML/PDF to text.
\end{itemize}
Each source is implemented as an adapter with: (i) request policy (headers, retries, backoff), (ii) parsing (JSON/XML/HTML), (iii) field mapping into the normalization schema, (iv) provenance capture (URL, timestamp, ETag/Last-Modified).

\subsection{Normalization Pipeline}
The pipeline applies a deterministic sequence:
\begin{enumerate}
  \item \textbf{Acquisition}: Pull using source adapter with exponential backoff and per-domain rate limits (default 1--5 RPS; burst control, token bucket).
  \item \textbf{Sanitization}: Strip HTML, decode entities, remove boilerplate, drop scripts/styles, normalize whitespace and Unicode.
  \item \textbf{Document Splitting}: For long artifacts, chunk by semantic/structural cues (headings, sections, bullet lists) with overlap to preserve context.
  \item \textbf{Canonicalization}: Normalize dates (ISO-8601), severity fields (CVSS base), product names, and IDs (CVE, CWE). Map tactics/techniques to ATT\&CK identifiers where present \cite{mitre_attack}.
  \item \textbf{De-duplication}: Exact dupes via content hash (SHA-256 of canonical text); near-duplication via MinHash/Jaccard threshold on shingles (default 0.85).
  \item \textbf{Validation}: Schema validation (required fields), reject malformed items, attach validation errors to audit log.
  \item \textbf{Publication}: Emit normalized records onto the message bus for downstream agents; store raw + normalized in object storage with immutable IDs.
\end{enumerate}

\subsection{Normalization JSON Schema (excerpt)}
We adopt a compact JSON record that preserves both canonical fields and provenance. (Shown as an illustrative excerpt.)
\begin{verbatim}
{
  "id": "doc-2025-09-30T12:34:56Z-000123",
  "source": {
    "name": "CISA KEV",
    "url": "https://www.cisa.gov/known-exploited-...",
    "retrieved_at": "2025-09-30T12:35:01Z",
    "etag": "W/\"advisory-...\""
  },
  "entities_hint": ["CVE-2025-12345", "Cisco IOS XE"],
  "published_at": "2025-09-29T00:00:00Z",
  "title": "Exploited Vulnerability in ...",
  "body": "Full cleaned text ...",
  "metadata": {
    "cve_ids": ["CVE-2025-12345"],
    "cvss_base": 9.8,
    "kev_listed": true,
    "attck": ["T1190"],
    "vendors": ["Cisco"],
    "products": ["IOS XE"]
  },
  "hashes": {
    "raw_sha256": "...",
    "canonical_sha256": "..."
  }
}
\end{verbatim}

\subsection{Rate Limiting, Retries, \& Backoff}
Adapters enforce per-host RPS caps and concurrent connection limits. Transient failures (HTTP 429/5xx) trigger exponential backoff with jitter; persistent failures are quarantined and surfaced as health metrics.

\subsection{De-duplication \& Canonical Selection}
When multiple sources report the same fact, records are clustered using canonical text hashes and near-dup similarity. The cluster representative is chosen by a source-precedence policy (official feeds $>$ vendor advisories $>$ blogs) and recency, while retaining all provenance for explainability.

\subsection{Quality Gates \& Monitoring}
We log ingestion success/failure rates, schema validation errors, de-dup ratios, byte/record throughput, median/95p latency, and per-source health. These feed dashboards and alerts consumed by Agent D and the operations team.

\subsection{Outputs}
Agent A outputs \emph{normalized, de-duplicated, provenance-rich} CTI documents into the bus/storage, which Agent B consumes for entity extraction and indexing.
